var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#DecisionFocusedLearningAlgorithms.AbstractAlgorithm","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.AbstractAlgorithm","text":"abstract type AbstractAlgorithm\n\nAn abstract type for decision-focused learning algorithms.\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.AbstractImitationAlgorithm","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.AbstractImitationAlgorithm","text":"abstract type AbstractImitationAlgorithm <: DecisionFocusedLearningAlgorithms.AbstractAlgorithm\n\nAn abstract type for imitation learning algorithms.\n\nAll subtypes must implement:\n\ntrain_policy!(algorithm::AbstractImitationAlgorithm, model, maximizer, train_data; epochs, metrics)\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.AbstractMetric","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.AbstractMetric","text":"abstract type AbstractMetric\n\nAbstract base type for all metrics used during training.\n\nAll concrete metric types should implement:\n\nevaluate!(metric, context) - Evaluate the metric given a training context\n\nSee also\n\nLossAccumulator\nFYLLossMetric\nFunctionMetric\nPeriodicMetric\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.AbstractPolicy","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.AbstractPolicy","text":"abstract type AbstractPolicy\n\nAbstract type for policies used in decision-focused learning.\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.AnticipativeImitation","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.AnticipativeImitation","text":"struct AnticipativeImitation{A} <: DecisionFocusedLearningAlgorithms.AbstractImitationAlgorithm\n\nAnticipative Imitation algorithm for supervised learning using anticipative solutions.\n\nTrains a policy in a single shot using expert demonstrations from anticipative solutions.\n\nReference: https://arxiv.org/abs/2304.00789\n\nFields\n\ninner_algorithm::Any: inner imitation algorithm for supervised learning\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.DAgger","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.DAgger","text":"struct DAgger{A} <: DecisionFocusedLearningAlgorithms.AbstractImitationAlgorithm\n\nDataset Aggregation (DAgger) algorithm for imitation learning.\n\nReference: https://arxiv.org/abs/2402.04463\n\nFields\n\ninner_algorithm::Any: inner imitation algorithm for supervised learning\niterations::Int64: number of DAgger iterations\nepochs_per_iteration::Int64: number of epochs per DAgger iteration\nα_decay::Float64: decay factor for mixing expert and learned policy\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.DFLPolicy","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.DFLPolicy","text":"struct DFLPolicy{ML, CO} <: AbstractPolicy\n\nDecision-Focused Learning Policy combining a machine learning model and a combinatorial optimizer.\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.DFLPolicy-Tuple{AbstractArray}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.DFLPolicy","text":"Run the policy and get the next decision on the given input features.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.FYLLossMetric","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.FYLLossMetric","text":"struct FYLLossMetric{D} <: AbstractMetric\n\nMetric for evaluating Fenchel-Young Loss over a dataset.\n\nThis metric stores a dataset and computes the average Fenchel-Young Loss when evaluate! is called. Useful for tracking validation loss during training. Can also be used in the algorithms to accumulate loss over training data with update!.\n\nFields\n\ndataset::Any: dataset to evaluate on\naccumulator::LossAccumulator: accumulator for loss values\n\nExamples\n\n# Create metric with validation dataset\nval_metric = FYLLossMetric(val_dataset, :validation_loss)\n\n# Evaluate during training (called by evaluate_metrics!)\ncontext = TrainingContext(policy=policy, epoch=5, loss=loss)\navg_loss = evaluate!(val_metric, context)\n\nSee also\n\nLossAccumulator\nFunctionMetric\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.FYLLossMetric-2","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.FYLLossMetric","text":"FYLLossMetric(dataset, name::Symbol=:fyl_loss)\n\nConstruct a FYLLossMetric for a given dataset.\n\nArguments\n\ndataset - Dataset to evaluate on (should have samples with .x, .y, and .info fields)\nname::Symbol - Identifier for the metric (default: :fyl_loss)\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.FunctionMetric","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.FunctionMetric","text":"struct FunctionMetric{F, D} <: AbstractMetric\n\nA flexible metric that wraps a user-defined function.\n\nThis metric allows users to define custom metrics using functions. The function receives the training context and optionally any stored data. It can return:\n\nA single value (stored with metric.name)\nA NamedTuple (each key-value pair stored separately)\n\nFields\n\nmetric_fn::Any: function with signature (context) -> value or (context, data) -> value\nname::Symbol: identifier for the metric\ndata::Any: optional data stored in the metric (default: nothing)\n\nExamples\n\n# Simple metric using only context\nepoch_metric = FunctionMetric(ctx -> ctx.epoch, :current_epoch)\n\n# Metric with stored data (dataset)\ngap_metric = FunctionMetric(:val_gap, val_data) do ctx, data\n    compute_gap(benchmark, data, ctx.model, ctx.maximizer)\nend\n\n# Metric returning multiple values\ndual_gap = FunctionMetric(:gaps, (train_data, val_data)) do ctx, datasets\n    train_ds, val_ds = datasets\n    return (\n        train_gap = compute_gap(benchmark, train_ds, ctx.model, ctx.maximizer),\n        val_gap = compute_gap(benchmark, val_ds, ctx.model, ctx.maximizer)\n    )\nend\n\nSee also\n\nPeriodicMetric - Wrap a metric to evaluate periodically\nevaluate!\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.FunctionMetric-Union{Tuple{F}, Tuple{F, Symbol}} where F","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.FunctionMetric","text":"FunctionMetric(\n    metric_fn,\n    name::Symbol\n) -> FunctionMetric{_A, Nothing} where _A\n\n\nConstruct a FunctionMetric without stored data.\n\nThe function should have signature (context) -> value.\n\nArguments\n\nmetric_fn::Function - Function to compute the metric\nname::Symbol - Identifier for the metric\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.LossAccumulator","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.LossAccumulator","text":"LossAccumulator() -> LossAccumulator\nLossAccumulator(name::Symbol) -> LossAccumulator\n\n\nConstruct a LossAccumulator with the given name. Initializes total loss and count to zero.\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.LossAccumulator-2","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.LossAccumulator","text":"mutable struct LossAccumulator\n\nAccumulates loss values during training and computes their average.\n\nThis metric is used internally by training loops to track training loss. It accumulates loss values via update! calls and computes the average via compute!.\n\nFields\n\nname::Symbol\ntotal_loss::Float64: Running sum of loss values\ncount::Int64: Number of samples accumulated\n\nExamples\n\nmetric = LossAccumulator(:training_loss)\n\n# During training\nfor sample in dataset\n    loss_value = compute_loss(model, sample)\n    update!(metric, loss_value)\nend\n\n# Get average and reset\navg_loss = compute!(metric)  # Automatically resets\n\nSee also\n\nFYLLossMetric\nreset!\nupdate!\ncompute!\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.PeriodicMetric","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.PeriodicMetric","text":"struct PeriodicMetric{M<:AbstractMetric} <: AbstractMetric\n\nWrapper that evaluates a metric only every N epochs.\n\nThis is useful for expensive metrics that don't need to be computed every epoch (e.g., gap computation, test set evaluation).\n\nFields\n\nmetric::AbstractMetric: the wrapped metric to evaluate periodically\nfrequency::Int64: evaluate every N epochs\noffset::Int64: offset for the first evaluation\n\nBehavior\n\nThe metric is evaluated when (epoch - offset) % frequency == 0. On other epochs, evaluate! returns nothing (which is skipped by evaluate_metrics!).\n\nSee also\n\nFunctionMetric\nevaluate!\nevaluate_metrics!\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.PeriodicMetric-Tuple{Any, Int64}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.PeriodicMetric","text":"PeriodicMetric(\n    metric_fn,\n    frequency::Int64;\n    offset\n) -> PeriodicMetric\n\n\nConstruct a PeriodicMetric from a function to be wrapped.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.PeriodicMetric-Union{Tuple{M}, Tuple{M, Int64}} where M<:AbstractMetric","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.PeriodicMetric","text":"PeriodicMetric(\n    metric::AbstractMetric,\n    frequency::Int64;\n    offset\n) -> PeriodicMetric\n\n\nConstruct a PeriodicMetric that evaluates the wrapped metric every N epochs.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.PerturbedFenchelYoungLossImitation","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.PerturbedFenchelYoungLossImitation","text":"struct PerturbedFenchelYoungLossImitation{O, S} <: DecisionFocusedLearningAlgorithms.AbstractImitationAlgorithm\n\nStructured imitation learning with a perturbed Fenchel-Young loss.\n\nReference: https://arxiv.org/abs/2002.08676\n\nFields\n\nnb_samples::Int64: number of perturbation samples\nε::Float64: perturbation magnitude\nthreaded::Bool: whether to use threading for perturbations\ntraining_optimizer::Any: optimizer used for training\nseed::Any: random seed for perturbations\nuse_multiplicative_perturbation::Bool: whether to use multiplicative perturbation (else additive)\n\n\n\n\n\n","category":"type"},{"location":"api/#DecisionFocusedLearningAlgorithms.TrainingContext","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.TrainingContext","text":"mutable struct TrainingContext{P, F, O<:NamedTuple}\n\nLightweight mutable context object passed to metrics during training.\n\nFields\n\npolicy::Any\nepoch::Int64: current epoch number (mutated in-place during training)\nmaximizer_kwargs::Any\nother_fields::NamedTuple\n\nNotes\n\npolicy, maximizer_kwargs, and other_fields are constant after construction; only epoch is intended to be mutated.\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.getproperty-Tuple{PeriodicMetric, Symbol}","page":"API Reference","title":"Base.getproperty","text":"getproperty(pm::PeriodicMetric, s::Symbol) -> Any\n\n\nDelegate name property to the wrapped metric for seamless integration.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.propertynames","page":"API Reference","title":"Base.propertynames","text":"propertynames(pm::PeriodicMetric) -> NTuple{4, Symbol}\npropertynames(\n    pm::PeriodicMetric,\n    private::Bool\n) -> NTuple{4, Symbol}\n\n\nList available properties of PeriodicMetric.\n\n\n\n\n\n","category":"function"},{"location":"api/#DecisionFocusedLearningAlgorithms._store_metric_value!-Tuple{ValueHistories.MVHistory, Symbol, Int64, NamedTuple}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms._store_metric_value!","text":"_store_metric_value!(\n    history::ValueHistories.MVHistory,\n    _::Symbol,\n    epoch::Int64,\n    value::NamedTuple\n)\n\n\nInternal helper to store multiple metric values from a NamedTuple. Each key-value pair is stored separately in the history.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms._store_metric_value!-Tuple{ValueHistories.MVHistory, Symbol, Int64, Nothing}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms._store_metric_value!","text":"_store_metric_value!(\n    _::ValueHistories.MVHistory,\n    _::Symbol,\n    _::Int64,\n    _::Nothing\n)\n\n\nInternal helper that skips storing when value is nothing. Used by periodic metrics on epochs when they're not evaluated.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms._store_metric_value!-Tuple{ValueHistories.MVHistory, Symbol, Int64, Number}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms._store_metric_value!","text":"_store_metric_value!(\n    history::ValueHistories.MVHistory,\n    metric_name::Symbol,\n    epoch::Int64,\n    value::Number\n)\n\n\nInternal helper to store a single metric value in the history.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.compute!-Tuple{FYLLossMetric}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.compute!","text":"compute!(metric::FYLLossMetric) -> Float64\n\n\nCompute the average loss from accumulated values.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.compute!-Tuple{LossAccumulator}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.compute!","text":"compute!(metric::LossAccumulator; reset) -> Float64\n\n\nCompute the average loss from accumulated values.\n\nArguments\n\nmetric::LossAccumulator - The accumulator to compute from\nreset::Bool - Whether to reset the accumulator after computing (default: true)\n\nReturns\n\nFloat64 - Average loss (or 0.0 if no values accumulated)\n\nExamples\n\nmetric = LossAccumulator()\nupdate!(metric, 1.5)\nupdate!(metric, 2.5)\navg = compute!(metric)  # Returns 2.0, then resets\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.evaluate!","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.evaluate!","text":"evaluate!(metric::AbstractMetric, context::TrainingContext)\n\nEvaluate the metric given the current training context.\n\nArguments\n\nmetric::AbstractMetric - The metric to evaluate\ncontext::TrainingContext - Current training state (model, epoch, maximizer, etc.)\n\nReturns\n\nCan return:\n\nA single value (Float64, Int, etc.) - stored with metric.name\nA NamedTuple - each key-value pair stored separately\nnothing - skipped (e.g., periodic metrics on off-epochs)\n\n\n\n\n\n","category":"function"},{"location":"api/#DecisionFocusedLearningAlgorithms.evaluate!-Tuple{FYLLossMetric, TrainingContext}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.evaluate!","text":"evaluate!(\n    metric::FYLLossMetric,\n    context::TrainingContext\n) -> Float64\n\n\nEvaluate the average Fenchel-Young Loss over the stored dataset.\n\nThis method iterates through the dataset, computes predictions using context.policy, and accumulates losses using context.loss. The dataset should be stored in the metric.\n\nArguments\n\nmetric::FYLLossMetric - The metric to evaluate\ncontext - TrainingContext with policy, loss, and other fields\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.evaluate!-Tuple{FunctionMetric, TrainingContext}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.evaluate!","text":"evaluate!(\n    metric::FunctionMetric,\n    context::TrainingContext\n) -> Any\n\n\nEvaluate the function metric by calling the stored function.\n\nArguments\n\nmetric::FunctionMetric - The metric to evaluate\ncontext - TrainingContext with current training state\n\nReturns\n\nThe value returned by metric.metric_fn (can be single value or NamedTuple)\n\n```\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.evaluate!-Tuple{PeriodicMetric, Any}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.evaluate!","text":"evaluate!(pm::PeriodicMetric, context) -> Any\n\n\nEvaluate the wrapped metric only if the current epoch matches the frequency pattern.\n\nArguments\n\npm::PeriodicMetric - The periodic metric wrapper\ncontext - TrainingContext with current epoch\n\nReturns\n\nThe result of evaluate!(pm.metric, context) if epoch matches the pattern\nnothing otherwise (which is skipped by evaluate_metrics!)\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.evaluate_metrics!-Tuple{ValueHistories.MVHistory, Tuple, TrainingContext}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.evaluate_metrics!","text":"evaluate_metrics!(\n    history::ValueHistories.MVHistory,\n    metrics::Tuple,\n    context::TrainingContext\n)\n\n\nEvaluate all metrics and store their results in the history.\n\nThis function handles three types of metric returns through multiple dispatch:\n\nSingle value: Stored with the metric's name\nNamedTuple: Each key-value pair stored separately (for metrics that compute multiple values)\nnothing: Skipped (e.g., periodic metrics on epochs when not evaluated)\n\nArguments\n\nhistory::MVHistory - MVHistory object to store metric values\nmetrics::Tuple - Tuple of AbstractMetric instances to evaluate\ncontext::TrainingContext - TrainingContext with current training state (policy, epoch, etc.)\n\nExamples\n\n# Create metrics\nval_loss = FYLLossMetric(val_dataset, :validation_loss)\nepoch_metric = FunctionMetric(ctx -> ctx.epoch, :current_epoch)\n\n# Evaluate and store\ncontext = TrainingContext(policy=policy, epoch=5)\nevaluate_metrics!(history, (val_loss, epoch_metric), context)\n\nSee also\n\nAbstractMetric\nevaluate!\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.next_epoch!-Tuple{TrainingContext}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.next_epoch!","text":"next_epoch!(ctx::TrainingContext)\n\n\nAdvance the epoch counter in the training context by one.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.reset!-Tuple{FYLLossMetric}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.reset!","text":"reset!(metric::FYLLossMetric) -> Int64\n\n\nReset the metric's accumulated loss to zero.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.reset!-Tuple{LossAccumulator}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.reset!","text":"reset!(metric::LossAccumulator) -> Int64\n\n\nReset the accumulator to its initial state (zero total loss and count).\n\nExamples\n\nmetric = LossAccumulator()\nupdate!(metric, 1.5)\nupdate!(metric, 2.0)\nreset!(metric)  # total_loss = 0.0, count = 0\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.train_policy!-Tuple{AnticipativeImitation, DFLPolicy, Any}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.train_policy!","text":"train_policy!(\n    algorithm::AnticipativeImitation,\n    policy::DFLPolicy,\n    train_environments;\n    anticipative_policy,\n    epochs,\n    metrics,\n    maximizer_kwargs\n)\n\n\nTrain a DFLPolicy using the Anticipative Imitation algorithm on provided training environments.\n\nCore training method\n\nGenerates anticipative solutions from environments and trains the policy using supervised learning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.train_policy!-Tuple{DAgger, DFLPolicy, Any}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.train_policy!","text":"train_policy!(\n    algorithm::DAgger,\n    policy::DFLPolicy,\n    train_environments;\n    anticipative_policy,\n    metrics,\n    maximizer_kwargs\n)\n\n\nTrain a DFLPolicy using the DAgger algorithm on the provided training environments.\n\nCore training method\n\nRequires train_environments and anticipative_policy as keyword arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.train_policy!-Tuple{PerturbedFenchelYoungLossImitation, DFLPolicy, AbstractArray{<:DecisionFocusedLearningBenchmarks.Utils.DataSample}}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.train_policy!","text":"train_policy!(\n    algorithm::PerturbedFenchelYoungLossImitation,\n    policy::DFLPolicy,\n    train_dataset::AbstractArray{<:DecisionFocusedLearningBenchmarks.Utils.DataSample};\n    epochs,\n    metrics,\n    maximizer_kwargs\n) -> ValueHistories.MVHistory{ValueHistories.History}\n\n\nTrain a DFLPolicy using the Perturbed Fenchel-Young Loss Imitation Algorithm with unbatched data.\n\nThis convenience method wraps the dataset in a DataLoader with batchsize=1 and delegates  to the batched training method. For custom batching behavior, create your own DataLoader  and use the batched method directly.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.train_policy!-Tuple{PerturbedFenchelYoungLossImitation, DFLPolicy, MLUtils.DataLoader}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.train_policy!","text":"train_policy!(\n    algorithm::PerturbedFenchelYoungLossImitation,\n    policy::DFLPolicy,\n    train_dataset::MLUtils.DataLoader;\n    epochs,\n    metrics,\n    maximizer_kwargs\n) -> ValueHistories.MVHistory{ValueHistories.History}\n\n\nTrain a DFLPolicy using the Perturbed Fenchel-Young Loss Imitation Algorithm.\n\nThe train_dataset should be a DataLoader for batched training. Gradients are computed  from the sum of losses across each batch before updating model parameters.\n\nFor unbatched training with a Vector{DataSample}, use the convenience method that  automatically wraps the data in a DataLoader with batchsize=1.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.train_policy-Tuple{AnticipativeImitation, DecisionFocusedLearningBenchmarks.Utils.AbstractStochasticBenchmark{true}}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.train_policy","text":"train_policy(\n    algorithm::AnticipativeImitation,\n    benchmark::DecisionFocusedLearningBenchmarks.Utils.AbstractStochasticBenchmark{true};\n    dataset_size,\n    split_ratio,\n    epochs,\n    metrics,\n    seed\n) -> Tuple{Any, DFLPolicy}\n\n\nTrain a DFLPolicy using the Anticipative Imitation algorithm on a benchmark.\n\nBenchmark convenience wrapper\n\nThis high-level function handles all setup from the benchmark and returns a trained policy. Uses anticipative solutions as expert demonstrations.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.train_policy-Tuple{DAgger, DecisionFocusedLearningBenchmarks.Utils.AbstractStochasticBenchmark{true}}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.train_policy","text":"train_policy(\n    algorithm::DAgger,\n    benchmark::DecisionFocusedLearningBenchmarks.Utils.AbstractStochasticBenchmark{true};\n    dataset_size,\n    split_ratio,\n    metrics,\n    seed\n) -> Tuple{ValueHistories.MVHistory{ValueHistories.History}, DFLPolicy}\n\n\nTrain a DFLPolicy using the DAgger algorithm on a benchmark.\n\nBenchmark convenience wrapper\n\nThis high-level function handles all setup from the benchmark and returns a trained policy.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.train_policy-Tuple{PerturbedFenchelYoungLossImitation, DecisionFocusedLearningBenchmarks.Utils.AbstractBenchmark}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.train_policy","text":"train_policy(\n    algorithm::PerturbedFenchelYoungLossImitation,\n    benchmark::DecisionFocusedLearningBenchmarks.Utils.AbstractBenchmark;\n    dataset_size,\n    split_ratio,\n    epochs,\n    metrics,\n    seed\n) -> Tuple{ValueHistories.MVHistory{ValueHistories.History}, DFLPolicy}\n\n\nTrain a DFLPolicy using the Perturbed Fenchel-Young Loss Imitation Algorithm on a benchmark.\n\nBenchmark convenience wrapper\n\nThis high-level function handles all setup from the benchmark and returns a trained policy.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.update!-Tuple{FYLLossMetric, Float64}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.update!","text":"update!(\n    metric::FYLLossMetric,\n    loss_value::Float64\n) -> Float64\n\n\nUpdate the metric with an already-computed loss value. This avoids re-evaluating the loss inside the metric when the loss was computed during training.\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.update!-Tuple{FYLLossMetric, InferOpt.FenchelYoungLoss, Any, Any}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.update!","text":"update!(\n    metric::FYLLossMetric,\n    loss::InferOpt.FenchelYoungLoss,\n    θ,\n    y_target;\n    kwargs...\n) -> Any\n\n\nUpdate the metric with a single loss computation.\n\nArguments\n\nmetric::FYLLossMetric - The metric to update\nloss::FenchelYoungLoss - Loss function to use\nθ - Model prediction\ny_target - Target value\nkwargs... - Additional arguments passed to loss function\n\n\n\n\n\n","category":"method"},{"location":"api/#DecisionFocusedLearningAlgorithms.update!-Tuple{LossAccumulator, Float64}","page":"API Reference","title":"DecisionFocusedLearningAlgorithms.update!","text":"update!(\n    metric::LossAccumulator,\n    loss_value::Float64\n) -> Int64\n\n\nAdd a loss value to the accumulator.\n\nExamples\n\nmetric = LossAccumulator()\nupdate!(metric, 1.5)\nupdate!(metric, 2.0)\ncompute!(metric)  # Returns 1.75\n\n\n\n\n\n","category":"method"},{"location":"interface/#Algorithm-Interface","page":"Interface Guide","title":"Algorithm Interface","text":"This page describes the unified interface for Decision-Focused Learning algorithms provided by this package.","category":"section"},{"location":"interface/#Core-Concepts","page":"Interface Guide","title":"Core Concepts","text":"","category":"section"},{"location":"interface/#DFLPolicy","page":"Interface Guide","title":"DFLPolicy","text":"The DFLPolicy is the central abstraction that encapsulates a decision-focused learning policy. It combines:\n\nA statistical model (typically a neural network) that predicts parameters from input features\nA combinatorial optimizer (maximizer) that solves optimization problems using the predicted parameters\n\npolicy = DFLPolicy(\n    Chain(Dense(input_dim => hidden_dim, relu), Dense(hidden_dim => output_dim)),\n    my_optimizer\n)","category":"section"},{"location":"interface/#Training-Interface","page":"Interface Guide","title":"Training Interface","text":"All algorithms in this package follow a unified training interface with two main functions:","category":"section"},{"location":"interface/#Core-Training-Method","page":"Interface Guide","title":"Core Training Method","text":"history = train_policy!(algorithm, policy, training_data; epochs=100, metrics=(), maximizer_kwargs=get_info)\n\nArguments:\n\nalgorithm: An algorithm instance (e.g., PerturbedFenchelYoungLossImitation, DAgger, AnticipativeImitation)\npolicy::DFLPolicy: The policy to train (contains the model and maximizer)\ntraining_data: Either a dataset of DataSample objects or Environment (depends on algorithm)\nepochs::Int: Number of training epochs (default: 100)\nmetrics::Tuple: Metrics to evaluate during training (default: empty)\nmaximizer_kwargs::Function: Function that extracts keyword arguments for the maximizer from data samples (default: get_info)\n\nReturns:\n\nhistory::MVHistory: Training history containing loss values and metric evaluations","category":"section"},{"location":"interface/#Benchmark-Convenience-Wrapper","page":"Interface Guide","title":"Benchmark Convenience Wrapper","text":"result = train_policy(algorithm, benchmark; dataset_size=30, split_ratio=(0.3, 0.3), epochs=100, metrics=())\n\nThis high-level function handles all setup from a benchmark and returns a trained policy along with training history.\n\nArguments:\n\nalgorithm: An algorithm instance\nbenchmark::AbstractBenchmark: A benchmark from DecisionFocusedLearningBenchmarks.jl\ndataset_size::Int: Number of instances to generate\nsplit_ratio::Tuple: Train/validation/test split ratios\nepochs::Int: Number of training epochs\nmetrics::Tuple: Metrics to track during training\n\nReturns:\n\n(; policy, history): Named tuple with trained policy and training history","category":"section"},{"location":"interface/#Metrics","page":"Interface Guide","title":"Metrics","text":"Metrics allow you to track additional quantities during training.","category":"section"},{"location":"interface/#Built-in-Metrics","page":"Interface Guide","title":"Built-in Metrics","text":"","category":"section"},{"location":"interface/#FYLLossMetric","page":"Interface Guide","title":"FYLLossMetric","text":"Evaluates Fenchel-Young loss on a validation dataset.\n\nval_metric = FYLLossMetric(validation_data, :validation_loss)","category":"section"},{"location":"interface/#FunctionMetric","page":"Interface Guide","title":"FunctionMetric","text":"Custom metric defined by a function.\n\n# Simple metric (no stored data)\nepoch_metric = FunctionMetric(ctx -> ctx.epoch, :epoch)\n\n# Metric with stored data\ngap_metric = FunctionMetric(:validation_gap, validation_data) do ctx, data\n    compute_gap(benchmark, data, ctx.policy.statistical_model, ctx.policy.maximizer)\nend","category":"section"},{"location":"interface/#TrainingContext","page":"Interface Guide","title":"TrainingContext","text":"Metrics receive a TrainingContext object containing:\n\npolicy::DFLPolicy: The policy being trained\nepoch::Int: Current epoch number\nmaximizer_kwargs::Function: Maximizer kwargs extractor\nother_fields: Algorithm-specific fields (e.g., loss for FYL)\n\nAccess policy components:\n\nctx.policy.statistical_model  # Neural network\nctx.policy.maximizer          # Combinatorial optimizer","category":"section"},{"location":"tutorials/tutorial/#Basic-Tutorial:-Training-with-FYL-on-Argmax-Benchmark","page":"Basic Tutorial: Training with FYL on Argmax Benchmark","title":"Basic Tutorial: Training with FYL on Argmax Benchmark","text":"This tutorial demonstrates the basic workflow for training a policy using the Perturbed Fenchel-Young Loss algorithm.","category":"section"},{"location":"tutorials/tutorial/#Setup","page":"Basic Tutorial: Training with FYL on Argmax Benchmark","title":"Setup","text":"using DecisionFocusedLearningAlgorithms\nusing DecisionFocusedLearningBenchmarks\nusing MLUtils: splitobs\nusing Plots","category":"section"},{"location":"tutorials/tutorial/#Create-Benchmark-and-Data","page":"Basic Tutorial: Training with FYL on Argmax Benchmark","title":"Create Benchmark and Data","text":"b = ArgmaxBenchmark()\ndataset = generate_dataset(b, 100)\ntrain_data, val_data, test_data = splitobs(dataset; at=(0.3, 0.3, 0.4))","category":"section"},{"location":"tutorials/tutorial/#Create-Policy","page":"Basic Tutorial: Training with FYL on Argmax Benchmark","title":"Create Policy","text":"model = generate_statistical_model(b; seed=0)\nmaximizer = generate_maximizer(b)\npolicy = DFLPolicy(model, maximizer)","category":"section"},{"location":"tutorials/tutorial/#Configure-Algorithm","page":"Basic Tutorial: Training with FYL on Argmax Benchmark","title":"Configure Algorithm","text":"algorithm = PerturbedFenchelYoungLossImitation(;\n    nb_samples=10, ε=0.1, threaded=true, seed=0\n)","category":"section"},{"location":"tutorials/tutorial/#Define-Metrics-to-track-during-training","page":"Basic Tutorial: Training with FYL on Argmax Benchmark","title":"Define Metrics to track during training","text":"validation_loss_metric = FYLLossMetric(val_data, :validation_loss)\n\nval_gap_metric = FunctionMetric(:val_gap, val_data) do ctx, data\n    compute_gap(b, data, ctx.policy.statistical_model, ctx.policy.maximizer)\nend\n\ntest_gap_metric = FunctionMetric(:test_gap, test_data) do ctx, data\n    compute_gap(b, data, ctx.policy.statistical_model, ctx.policy.maximizer)\nend\n\nmetrics = (validation_loss_metric, val_gap_metric, test_gap_metric)","category":"section"},{"location":"tutorials/tutorial/#Train-the-Policy","page":"Basic Tutorial: Training with FYL on Argmax Benchmark","title":"Train the Policy","text":"history = train_policy!(algorithm, policy, train_data; epochs=100, metrics=metrics)","category":"section"},{"location":"tutorials/tutorial/#Plot-Results","page":"Basic Tutorial: Training with FYL on Argmax Benchmark","title":"Plot Results","text":"val_gap_epochs, val_gap_values = get(history, :val_gap)\ntest_gap_epochs, test_gap_values = get(history, :test_gap)\n\nplot(\n    [val_gap_epochs, test_gap_epochs],\n    [val_gap_values, test_gap_values];\n    labels=[\"Val Gap\" \"Test Gap\"],\n    xlabel=\"Epoch\",\n    ylabel=\"Gap\",\n    title=\"Gap Evolution During Training\",\n)\n\nPlot loss evolution\n\ntrain_loss_epochs, train_loss_values = get(history, :training_loss)\nval_loss_epochs, val_loss_values = get(history, :validation_loss)\n\nplot(\n    [train_loss_epochs, val_loss_epochs],\n    [train_loss_values, val_loss_values];\n    labels=[\"Training Loss\" \"Validation Loss\"],\n    xlabel=\"Epoch\",\n    ylabel=\"Loss\",\n    title=\"Loss Evolution During Training\",\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"tutorials/warcraft_fyl/#Training-on-Warcraft-Shortest-Path","page":"Training on Warcraft Shortest Path","title":"Training on Warcraft Shortest Path","text":"This tutorial demonstrates how to train a decision-focused learning policy on the Warcraft shortest path benchmark using the Perturbed Fenchel-Young Loss Imitation algorithm.","category":"section"},{"location":"tutorials/warcraft_fyl/#Setup","page":"Training on Warcraft Shortest Path","title":"Setup","text":"First, let's load the required packages:\n\nusing DecisionFocusedLearningAlgorithms\nusing DecisionFocusedLearningBenchmarks\nusing Flux\nusing MLUtils\nusing Plots\nusing Statistics","category":"section"},{"location":"tutorials/warcraft_fyl/#Benchmark-Setup","page":"Training on Warcraft Shortest Path","title":"Benchmark Setup","text":"The Warcraft benchmark involves predicting edge costs in a grid graph for shortest path problems. We'll create a benchmark instance and generate training data:\n\nbenchmark = WarcraftBenchmark()\ndataset = generate_dataset(benchmark, 50)\n\nSplit the dataset into training, validation, and test sets:\n\ntrain_data, val_data = dataset[1:45], dataset[46:end]","category":"section"},{"location":"tutorials/warcraft_fyl/#Creating-a-Policy","page":"Training on Warcraft Shortest Path","title":"Creating a Policy","text":"A DFLPolicy combines a statistical model (neural network) with a combinatorial optimizer. The benchmark provides utilities to generate appropriate models and optimizers:\n\nmodel = generate_statistical_model(benchmark)\nmaximizer = generate_maximizer(benchmark; dijkstra=true)\npolicy = DFLPolicy(model, maximizer)","category":"section"},{"location":"tutorials/warcraft_fyl/#Configuring-the-Algorithm","page":"Training on Warcraft Shortest Path","title":"Configuring the Algorithm","text":"We'll use the Perturbed Fenchel-Young Loss Imitation algorithm:\n\nalgorithm = PerturbedFenchelYoungLossImitation(;\n    nb_samples=100,          # Number of perturbation samples for gradient estimation\n    ε=0.2,                  # Perturbation magnitude\n    threaded=true,          # Use multi-threading for perturbations\n    training_optimizer=Adam(1e-3),  # Flux optimizer with learning rate\n    seed=42,                 # Random seed for reproducibility\n    use_multiplicative_perturbation=true,  # Use multiplicative perturbations\n)","category":"section"},{"location":"tutorials/warcraft_fyl/#Setting-Up-Metrics","page":"Training on Warcraft Shortest Path","title":"Setting Up Metrics","text":"We'll track several metrics during training:\n\nValidation loss metric\n\nval_loss_metric = FYLLossMetric(val_data, :validation_loss)\n\nValidation gap metric\n\nval_gap_metric = FunctionMetric(:val_gap, val_data) do ctx, data\n    compute_gap(benchmark, data, ctx.policy.statistical_model, ctx.policy.maximizer)\nend","category":"section"},{"location":"tutorials/warcraft_fyl/#Training","page":"Training on Warcraft Shortest Path","title":"Training","text":"Now we train the policy:\n\ndata_loader = DataLoader(train_data; batchsize=50)\nhistory = train_policy!(\n    algorithm, policy, data_loader; epochs=50, metrics=(val_loss_metric, val_gap_metric)\n)","category":"section"},{"location":"tutorials/warcraft_fyl/#Results-Analysis","page":"Training on Warcraft Shortest Path","title":"Results Analysis","text":"Let's examine the training progress:\n\nExtract training history\n\ntrain_loss_epochs, train_loss_values = get(history, :training_loss)\nval_loss_epochs, val_loss_values = get(history, :validation_loss)\nval_gap_epochs, val_gap_values = get(history, :val_gap)\n\nPlot training and validation loss\n\np1 = plot(\n    train_loss_epochs,\n    train_loss_values;\n    label=\"Training\",\n    xlabel=\"Epoch\",\n    ylabel=\"FYL Loss\",\n    title=\"Training Progress\",\n    linewidth=2,\n)\nplot!(p1, val_loss_epochs, val_loss_values; label=\"Validation\", linewidth=2)\n\nPlot gap evolution\n\np2 = plot(\n    val_gap_epochs,\n    val_gap_values;\n    label=\"Validation Gap\",\n    xlabel=\"Epoch\",\n    ylabel=\"Gap (Regret)\",\n    title=\"Decision Quality\",\n    linewidth=2,\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#DecisionFocusedLearningAlgorithms","page":"Home","title":"DecisionFocusedLearningAlgorithms","text":"Documentation for DecisionFocusedLearningAlgorithms.","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"This package provides a unified interface for training decision-focused learning algorithms that combine machine learning with combinatorial optimization. It implements several state-of-the-art algorithms for learning to predict parameters of optimization problems.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Unified Interface: Consistent API across all algorithms via train_policy!\nPolicy-Centric Design: DFLPolicy encapsulates statistical models and optimizers\nFlexible Metrics: Track custom metrics during training\nBenchmark Integration: Seamless integration with DecisionFocusedLearningBenchmarks.jl","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using DecisionFocusedLearningAlgorithms\nusing DecisionFocusedLearningBenchmarks\n\n# Create a policy\nbenchmark = ArgmaxBenchmark()\nmodel = generate_statistical_model(benchmark)\nmaximizer = generate_maximizer(benchmark)\npolicy = DFLPolicy(model, maximizer)\n\n# Train with FYL algorithm\nalgorithm = PerturbedFenchelYoungLossImitation()\nresult = train_policy(algorithm, benchmark; epochs=50)\n\nSee the Interface Guide and Tutorials for more details.","category":"section"},{"location":"#Available-Algorithms","page":"Home","title":"Available Algorithms","text":"Perturbed Fenchel-Young Loss Imitation: Differentiable imitation learning with perturbed optimization\nAnticipativeImitation: Imitation of anticipative solutions for dynamic problems\nDAgger: DAgger algorithm for dynamic problems","category":"section"}]
}
